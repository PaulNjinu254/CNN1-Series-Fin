{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvR/FfT/DRPueF8fG51VBd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulNjinu254/CNN1-Series-Fin/blob/main/CNN1_Series_Fin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8l4GJo9lg9U",
        "outputId": "c5e6b322-59c2-43d4-a4ac-8069a90b4807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dummy Forward/Backward Test with small array\n",
            "Forward output Z3 shape: (2, 2)\n",
            "\n",
            "Backward propagation successfully completed!\n",
            "\n",
            "Epoch 1 | Loss: 2.3023 | Accuracy: 0.1172\n",
            "Epoch 2 | Loss: 2.3021 | Accuracy: 0.1126\n",
            "Epoch 3 | Loss: 2.3018 | Accuracy: 0.1126\n",
            "Epoch 4 | Loss: 2.3016 | Accuracy: 0.1126\n",
            "Epoch 5 | Loss: 2.3014 | Accuracy: 0.1276\n",
            "\n",
            "Estimating MNIST with Conv1D replacing part of FC layer\n",
            "Accuracy: 0.1420\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Conv1D Layer\n",
        "class Conv1d:\n",
        "    def __init__(self, n_filters, b_size, stride=1, padding=0):\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "        self.stride = stride\n",
        "        self.pa = padding\n",
        "        self.b_size = b_size\n",
        "        self.n_filters = n_filters\n",
        "\n",
        "    def init_weights(self, C, b_size):\n",
        "        self.W = np.random.randn(self.n_filters, C, b_size) * 0.01\n",
        "        self.b = np.zeros((self.n_filters,))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = np.pad(x, ((0, 0), (0, 0), (self.pa, self.pa)), mode='constant')\n",
        "        N, C, L = self.x.shape\n",
        "        out_length = (L - self.b_size) // self.stride + 1\n",
        "        self.output_size = out_length\n",
        "\n",
        "        col = np.zeros((N, self.W.shape[0], out_length))\n",
        "        for i in range(out_length):\n",
        "            col[:, :, i] = np.tensordot(\n",
        "                self.x[:, :, i * self.stride:i * self.stride + self.b_size],\n",
        "                self.W, axes=([1, 2], [1, 2])\n",
        "            ) + self.b\n",
        "        self.col = col\n",
        "        return col\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        N, C, L = self.x.shape\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.sum(d_out, axis=(0, 2))\n",
        "\n",
        "        dx = np.zeros_like(self.x)\n",
        "        for i in range((L - self.b_size) // self.stride + 1):\n",
        "            window = self.x[:, :, i * self.stride:i * self.stride + self.b_size]\n",
        "            for n in range(N):\n",
        "                for f in range(self.n_filters):\n",
        "                    dW[f] += d_out[n, f, i] * window[n]\n",
        "                    dx[n, :, i * self.stride:i * self.stride + self.b_size] += d_out[n, f, i] * self.W[f]\n",
        "\n",
        "        self.dW = dW\n",
        "        self.db = db\n",
        "        return dx[:, :, self.pa:-self.pa] if self.pa > 0 else dx\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.W -= lr * self.dW\n",
        "        self.b -= lr * self.db\n",
        "\n",
        "# Fully Connected Layer\n",
        "class FC:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        self.W = np.random.randn(in_dim, out_dim) * 0.01\n",
        "        self.b = np.zeros((1, out_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.dot(x, self.W) + self.b\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        self.dW = np.dot(self.x.T, d_out)\n",
        "        self.db = np.sum(d_out, axis=0, keepdims=True)\n",
        "        return np.dot(d_out, self.W.T)\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.W -= lr * self.dW\n",
        "        self.b -= lr * self.db\n",
        "\n",
        "# Activation Functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-9)) / y_pred.shape[0]\n",
        "\n",
        "def delta_cross_entropy(y_pred, y_true):\n",
        "    return (y_pred - y_true) / y_true.shape[0]\n",
        "\n",
        "# CNN Classifier\n",
        "class Scratch1dCNNClassifier:\n",
        "    def __init__(self, num_epoch, lr, batch_size, n_features, n_nodes2, n_output, verbose=True):\n",
        "        self.num_epoch = num_epoch\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.conv = Conv1d(n_filters=6, b_size=3, padding=1)\n",
        "        self.conv.init_weights(1, 3)\n",
        "        self.fc1 = FC(6 * n_features, n_nodes2)\n",
        "        self.fc2 = FC(n_nodes2, n_output)\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        self.z1 = self.conv.forward(X)\n",
        "        self.a1 = relu(self.z1)\n",
        "        self.a1_flat = self.a1.reshape(X.shape[0], -1)\n",
        "        self.z2 = self.fc1.forward(self.a1_flat)\n",
        "        self.a2 = relu(self.z2)\n",
        "        self.z3 = self.fc2.forward(self.a2)\n",
        "        self.a3 = softmax(self.z3)\n",
        "        return self.a3\n",
        "\n",
        "    def back_propagation(self, y):\n",
        "        delta3 = delta_cross_entropy(self.a3, y)\n",
        "        delta2 = self.fc2.backward(delta3)\n",
        "        delta2 = delta2 * relu_grad(self.z2)\n",
        "        delta1 = self.fc1.backward(delta2)\n",
        "        delta1 = delta1.reshape(self.a1.shape)\n",
        "        delta0 = delta1 * relu_grad(self.z1)\n",
        "        self.conv.backward(delta0)\n",
        "        self.fc2.update(self.lr)\n",
        "        self.fc1.update(self.lr)\n",
        "        self.conv.update(self.lr)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for epoch in range(self.num_epoch):\n",
        "            perm = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[perm]\n",
        "            y_shuffled = y[perm]\n",
        "            for i in range(0, X.shape[0], self.batch_size):\n",
        "                X_batch = X_shuffled[i:i+self.batch_size]\n",
        "                y_batch = y_shuffled[i:i+self.batch_size]\n",
        "                self.forward_propagation(X_batch)\n",
        "                self.back_propagation(y_batch)\n",
        "            if self.verbose:\n",
        "                y_pred = self.forward_propagation(X)\n",
        "                loss = cross_entropy(y_pred, y)\n",
        "                acc = self.accuracy(y_pred, y)\n",
        "                print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward_propagation(X)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        "    def accuracy(self, y_pred, y_true):\n",
        "        return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))\n",
        "\n",
        "# Dummy Test\n",
        "print(\"\\nDummy Forward/Backward Test with small array\")\n",
        "dummy_X = np.random.rand(2, 1, 10)  # 2 samples, 1 channel, 10 length\n",
        "dummy_y = np.array([[1, 0], [0, 1]])  # one-hot encoded\n",
        "\n",
        "dummy_model = Scratch1dCNNClassifier(\n",
        "    num_epoch=1, lr=0.01, batch_size=2,\n",
        "    n_features=10, n_nodes2=5, n_output=2, verbose=False\n",
        ")\n",
        "dummy_model.fit(dummy_X, dummy_y)\n",
        "\n",
        "Z3 = dummy_model.forward_propagation(dummy_X)\n",
        "print(\"Forward output Z3 shape:\", Z3.shape)\n",
        "\n",
        "dummy_model.back_propagation(dummy_y)\n",
        "print(\"\\nBackward propagation successfully completed!\\n\")\n",
        "\n",
        "# Estimate MNIST using Conv1D replacing part of FC\n",
        "\n",
        "# Load MNIST and reshape properly (treat each row as a timestep)\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.astype(np.float32) / 255.0  # normalize\n",
        "X_test = X_test.astype(np.float32) / 255.0\n",
        "\n",
        "# Reshape for Conv1D: (batch, channels=1, length=28) treating each row (28 pixels) as a channel\n",
        "X_train = X_train.transpose(0, 2, 1)  # shape becomes (N, 28, 28)\n",
        "X_test = X_test.transpose(0, 2, 1)    # same\n",
        "\n",
        "# Now reshape to (N, 1, 28*28) for compatibility with your Conv1d (uses (N, C, L))\n",
        "X_train = X_train.reshape(-1, 1, 784)\n",
        "X_test = X_test.reshape(-1, 1, 784)\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_enc = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_enc = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Define improved model using Conv1d as feature extractor before FC\n",
        "class ConvReplacesFC_MNIST:\n",
        "    def __init__(self, num_epoch=5, lr=0.01, batch_size=128, kernel_size=5, n_filters=16, hidden_nodes=64, output_nodes=10, verbose=True):\n",
        "        self.num_epoch = num_epoch\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.conv = Conv1d(n_filters=n_filters, b_size=kernel_size, padding=2)\n",
        "        self.conv.init_weights(1, kernel_size)\n",
        "        self.fc1 = FC(n_filters * 784, hidden_nodes)\n",
        "        self.fc2 = FC(hidden_nodes, output_nodes)\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        self.z1 = self.conv.forward(X)\n",
        "        self.a1 = relu(self.z1)\n",
        "        self.a1_flat = self.a1.reshape(X.shape[0], -1)\n",
        "        self.z2 = self.fc1.forward(self.a1_flat)\n",
        "        self.a2 = relu(self.z2)\n",
        "        self.z3 = self.fc2.forward(self.a2)\n",
        "        self.a3 = softmax(self.z3)\n",
        "        return self.a3\n",
        "\n",
        "    def back_propagation(self, y_true):\n",
        "        delta3 = delta_cross_entropy(self.a3, y_true)\n",
        "        delta2 = self.fc2.backward(delta3)\n",
        "        delta2 *= relu_grad(self.z2)\n",
        "        delta1 = self.fc1.backward(delta2)\n",
        "        delta1 = delta1.reshape(self.a1.shape)\n",
        "        delta0 = delta1 * relu_grad(self.z1)\n",
        "        self.conv.backward(delta0)\n",
        "        self.fc2.update(self.lr)\n",
        "        self.fc1.update(self.lr)\n",
        "        self.conv.update(self.lr)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for epoch in range(self.num_epoch):\n",
        "            perm = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[perm]\n",
        "            y_shuffled = y[perm]\n",
        "            for i in range(0, X.shape[0], self.batch_size):\n",
        "                X_batch = X_shuffled[i:i+self.batch_size]\n",
        "                y_batch = y_shuffled[i:i+self.batch_size]\n",
        "                self.forward_propagation(X_batch)\n",
        "                self.back_propagation(y_batch)\n",
        "            if self.verbose:\n",
        "                y_pred = self.forward_propagation(X)\n",
        "                loss = cross_entropy(y_pred, y)\n",
        "                acc = self.accuracy(y_pred, y)\n",
        "                print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")\n",
        "\n",
        "    def accuracy(self, y_pred, y_true):\n",
        "        return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))\n",
        "\n",
        "# Train on a subset for fast test\n",
        "model = ConvReplacesFC_MNIST(num_epoch=5, lr=0.01, batch_size=128, verbose=True)\n",
        "model.fit(X_train[:5000], y_train_enc[:5000])  # faster training on subset\n",
        "\n",
        "# Evaluate on test data\n",
        "y_test_pred = model.forward_propagation(X_test[:1000])\n",
        "test_accuracy = model.accuracy(y_test_pred, y_test_enc[:1000])\n",
        "\n",
        "print(\"\\nEstimating MNIST with Conv1D replacing part of FC layer\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")"
      ]
    }
  ]
}